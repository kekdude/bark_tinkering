{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c9f743-a283-40ea-8aec-142f1590aabb",
   "metadata": {},
   "source": [
    "# Bark generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd653e6-52d6-4235-94a2-840920e7e149",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "\n",
    "There are a few ways to improve generation speed. Check Huggingface [article](https://huggingface.co/blog/optimizing-bark)\n",
    "\n",
    "Here we'll use 3 of them:\n",
    "1. Loading model in half-precision\n",
    "2. Using BetterTransformer to fuse some operations and make it faster\n",
    "3. Using batches\n",
    "\n",
    "Also remember that you can use bark-small instead of bark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6e1bd-5cf5-4349-802c-4567882769a3",
   "metadata": {},
   "source": [
    "### Load model in float16 and convert to BetterTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080eb4f-e375-4cc3-a35f-37676cec189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from transformers import BarkProcessor, BarkModel\n",
    "from bark_tinkering.adversarial_speaker import find_semantics_by_wav, \\\n",
    "    create_voice_preset, save_voice_preset_safetensors, load_voice_preset_safetensors, load_voice_preset_numpy\n",
    "from bark_tinkering.utils import make_text_generations, save_audio_from_generation\n",
    "import IPython\n",
    "\n",
    "device = 'cuda'\n",
    "transformers_cache_dir = '.cache'\n",
    "model_id = 'suno/bark'\n",
    "\n",
    "processor: BarkProcessor = BarkProcessor.from_pretrained(model_id, cache_dir=transformers_cache_dir)\n",
    "\n",
    "# Load model in float16\n",
    "model: BarkModel = BarkModel.from_pretrained(model_id, cache_dir=transformers_cache_dir, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Convert model to BetterTransformer\n",
    "model = BetterTransformer.transform(model, keep_original_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64b27a-a2c4-4240-9686-64bbd2bc1ca3",
   "metadata": {},
   "source": [
    "### Generate in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dc5f2-eede-4907-b0bc-22447358eb7f",
   "metadata": {},
   "source": [
    "Just pass your inputs to processor as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6e080-af3d-498e-b3e9-4ae89cc096f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_preset = load_voice_preset_safetensors('voice_presets/lina.safetensors')\n",
    "\n",
    "text_prompt = [\n",
    "    \"Let's try generating speech, with Bark, a text-to-speech model\",\n",
    "    \"Wow, batching is so great!\",\n",
    "    \"I love Hugging Face it's so cool.\"]\n",
    "\n",
    "inputs = processor(text_prompt, voice_preset=voice_preset).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66cb76-1c21-45e9-aae4-c36b03d6cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "with torch.inference_mode():\n",
    "  # samples are generated all at once\n",
    "  speech_output, speech_output_lenghts = model.generate(**inputs, \n",
    "                                 do_sample = True, \n",
    "                                 semantic_temperature=0.7, \n",
    "                                 coarse_temperature=1,\n",
    "                                 fine_temperature=0.5,\n",
    "                                 return_output_lengths=True, # Important! Get lengths to cut samples after generation\n",
    "                                 min_eos_p=0.05) # minimum probability of EOS token to stop generation and prevent Bark hallucinations\n",
    "\n",
    "for i in range(speech_output.size(0)):\n",
    "    audio = speech_output[i, :speech_output_lenghts[i]].cpu().numpy()\n",
    "    IPython.display.display(IPython.display.Audio(audio, rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009e046-328f-46fd-8dff-44b033a098a3",
   "metadata": {},
   "source": [
    "### Do the same thing with utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84dd7e-f574-40df-baff-fca28f1b3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark_tinkering.utils import bark_generate\n",
    "from notebook_utils import display_generation_result\n",
    "\n",
    "torch.manual_seed(2)\n",
    "with torch.inference_mode():\n",
    "    generations = bark_generate(model,\n",
    "                               **inputs, \n",
    "                               do_sample = True, \n",
    "                               semantic_temperature=0.7, \n",
    "                               coarse_temperature=1,\n",
    "                               fine_temperature=0.5,\n",
    "                               min_eos_p=0.05) # minimum probability of EOS token to stop generation and prevent Bark hallucinations\n",
    "\n",
    "display_generation_result(generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2a3dc-b5d7-4c90-8801-82876a016169",
   "metadata": {},
   "source": [
    "Save this sarcastic voiceline about batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0661e-b9b2-4f0b-88b3-2df825396164",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_audio_from_generation(model, generations[1], 'generations/sarcastic.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
